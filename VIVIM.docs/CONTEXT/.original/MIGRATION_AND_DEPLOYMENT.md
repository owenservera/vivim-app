# Migration and Deployment Guide

**Document Version:** 1.0.0
**Date:** February 11, 2026
**Related:** `IMPLEMENTATION_GUIDE_MASTER.md`, `DATABASE_SCHEMA_IMPLEMENTATION.md`

---

## Table of Contents

1. [Migration Overview](#migration-overview)
2. [Phase 1: Foundation (Safe Rollout)](#phase-1-foundation-safe-rollout)
3. [Phase 2: Activation (Beta Rollout)](#phase-2-activation-beta-rollout)
4. [Phase 3: Full Migration (Production)](#phase-3-full-migration-production)
5. [Database Migrations](#database-migrations)
6. [Environment Configuration](#environment-configuration)
7. [Deployment Procedures](#deployment-procedures)
8. [Rollback Strategies](#rollback-strategies)

---

## Migration Overview

### Migration Philosophy

**Gradual, Safe, Observable**

1. **Dual-Engine Architecture**: New and old engines run in parallel
2. **Feature Flag Control**: Toggle between engines without code deployment
3. **Graceful Fallback**: New engine failure automatically falls back to legacy
4. **Phased Rollout**: Test → Beta → Production with monitoring
5. **Rollback-Ready**: Ability to disable new engine instantly

### Migration Timeline

| Week | Phase | Activities | Success Criteria |
|-------|--------|-------------|-----------------|
| Week 1 | Foundation | Implement all services, schema, routes | All tests pass |
| Week 2-3 | Alpha (Internal) | Run with `USE_DYNAMIC_CONTEXT=false` | Zero incidents |
| Week 4-5 | Beta (Limited) | Enable for 5% of users | Cache hit rate > 70% |
| Week 6-7 | Production (Full) | Enable for 100% of users | Cache hit rate > 85% |
| Week 8+ | Optimization | Monitor, tune, refine | Context assembly < 100ms |

---

## Phase 1: Foundation (Safe Rollout)

### Objectives

1. **Implement all context engine components** without disrupting production
2. **Establish safe migration path** with `UnifiedContextService` bridge
3. **Ensure backward compatibility** with existing `context-generator.js`

### Pre-Migration Checklist

- [ ] Code changes reviewed by senior engineer
- [ ] All new services implement fallback to legacy system
- [ ] Feature flags implemented and documented
- [ ] Database migrations tested in staging
- [ ] Load testing completed (simulate 100 concurrent users)
- [ ] Monitoring dashboards configured
- [ ] Runbook documented for rollback procedures
- [ ] On-call team notified of deployment window

### Step-by-Step Implementation

#### Step 1: Database Schema Migration

```bash
# Navigate to server directory
cd server

# Generate Prisma client with new models
npx prisma generate

# Create migration
npx prisma migrate dev --name add_context_engine_tables

# Verify migration
npx prisma migrate dev --name verify_context_engine_schema

# Seed initial data (optional)
npx prisma db seed
```

**Migration Files:**

```sql
-- Migration: add_context_engine_tables.sql
-- Date: 2026-02-11

-- ═══════════════════════════════════════════
-- Enable pgvector extension
-- ═══════════════════════════════════════════
CREATE EXTENSION IF NOT EXISTS vector;

-- ═══════════════════════════════════════════════
-- Create TopicProfile table
-- ═════════════════════════════════════════════════════
CREATE TABLE "topic_profiles" (
  "id" TEXT NOT NULL,
  "userId" TEXT NOT NULL,
  "slug" TEXT NOT NULL,
  "label" TEXT NOT NULL,
  "aliases" TEXT[],
  "parentSlug" TEXT,
  "domain" TEXT NOT NULL,
  "totalConversations" INTEGER NOT NULL DEFAULT 0,
  "totalAcus" INTEGER NOT NULL DEFAULT 0,
  "totalMessages" INTEGER NOT NULL DEFAULT 0,
  "totalTokensSpent" INTEGER NOT NULL DEFAULT 0,
  "avgSessionDepth" DOUBLE PRECISION NOT NULL DEFAULT 0,
  "firstEngagedAt" TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  "lastEngagedAt" TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  "engagementStreak" INTEGER NOT NULL DEFAULT 0,
  "peakHour" INTEGER,
  "proficiencyLevel" TEXT NOT NULL DEFAULT 'unknown',
  "proficiencySignals" JSONB NOT NULL DEFAULT '[]',
  "importanceScore" DOUBLE PRECISION NOT NULL DEFAULT 0.5,
  "compiledContext" TEXT,
  "compiledAt" TIMESTAMPTZ,
  "compiledTokenCount" INTEGER,
  "contextVersion" INTEGER NOT NULL DEFAULT 0,
  "isDirty" BOOLEAN NOT NULL DEFAULT true,
  "embedding" vector(1536),
  "embeddingModel" TEXT,
  "createdAt" TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  "updatedAt" TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  "relatedMemoryIds" TEXT[] NOT NULL DEFAULT ARRAY[]::TEXT[],
  "relatedAcuIds" TEXT[] NOT NULL DEFAULT ARRAY[]::TEXT[],
  PRIMARY KEY ("id")
);

CREATE UNIQUE INDEX "topic_profiles_userId_slug_key" ON "topic_profiles"("userId", "slug");
CREATE INDEX "topic_profiles_importanceScore_idx" ON "topic_profiles"("importanceScore" DESC);
CREATE INDEX "topic_profiles_lastEngagedAt_idx" ON "topic_profiles"("lastEngagedAt" DESC);
CREATE INDEX "topic_profiles_isDirty_idx" ON "topic_profiles"("isDirty");
CREATE INDEX "topic_profiles_domain_idx" ON "topic_profiles"("domain");
CREATE INDEX "topic_profiles_embedding_idx" ON "topic_profiles" USING ivfflat("embedding" vector_cosine_ops) WITH (lists = 50);

-- ═════════════════════════════════════════════════════════════
-- Create EntityProfile table (similar structure)
-- ═════════════════════════════════════════════════════════════════════════
-- [EntityProfile table creation with indexes...]

-- ═════════════════════════════════════════════════════════════════════════════════
-- Create ContextBundle table
-- ═══════════════════════════════════════════════════════════════════════════════════════════════════════════
CREATE TABLE "context_bundles" (
  "id" TEXT NOT NULL,
  "userId" TEXT NOT NULL,
  "bundleType" TEXT NOT NULL,
  "topicProfileId" TEXT,
  "entityProfileId" TEXT,
  "conversationId" TEXT,
  "personaId" TEXT,
  "compiledPrompt" TEXT NOT NULL,
  "tokenCount" INTEGER NOT NULL,
  "composition" JSONB NOT NULL DEFAULT '{}',
  "version" INTEGER NOT NULL DEFAULT 1,
  "isDirty" BOOLEAN NOT NULL DEFAULT false,
  "priority" DOUBLE PRECISION NOT NULL DEFAULT 0.5,
  "compiledAt" TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  "expiresAt" TIMESTAMPTZ,
  "lastUsedAt" TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  "useCount" INTEGER NOT NULL DEFAULT 0,
  "hitCount" INTEGER NOT NULL DEFAULT 0,
  "missCount" INTEGER NOT NULL DEFAULT 0,
  PRIMARY KEY ("id")
);

CREATE INDEX "context_bundles_userId_bundleType_idx" ON "context_bundles"("userId", "bundleType");
CREATE INDEX "context_bundles_priority_idx" ON "context_bundles"("priority" DESC);
CREATE INDEX "context_bundles_isDirty_idx" ON "context_bundles"("isDirty");
CREATE INDEX "context_bundles_expiresAt_idx" ON "context_bundles"("expiresAt");

-- Foreign key constraints
ALTER TABLE "context_bundles" ADD CONSTRAINT "context_bundles_topicProfileId_fkey"
  FOREIGN KEY ("topicProfileId") REFERENCES "topic_profiles"("id") ON DELETE CASCADE;

ALTER TABLE "context_bundles" ADD CONSTRAINT "context_bundles_entityProfileId_fkey"
  FOREIGN KEY ("entityProfileId") REFERENCES "entity_profiles"("id") ON DELETE CASCADE;

ALTER TABLE "context_bundles" ADD CONSTRAINT "context_bundles_conversationId_fkey"
  FOREIGN KEY ("conversationId") REFERENCES "conversations"("id") ON DELETE CASCADE;

-- ═════════════════════════════════════════════════════════════════════════════════════════
-- Create ClientPresence table
-- ═════════════════════════════════════════════════════════════════════════════════════════════════════════════
-- [ClientPresence table creation...]

-- ═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════
```

**Verification Steps:**

```bash
# 1. Verify migration was applied
npx prisma migrate status

# 2. Check new tables exist
psql $DATABASE_URL -c "\dt topic_profiles; \d entity_profiles; \d context_bundles; \d client_presence;"

# 3. Verify indexes
psql $DATABASE_URL -c "\d topic_profiles"

# 4. Verify pgvector extension
psql $DATABASE_URL -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
```

---

## Phase 2: Activation (Beta Rollout)

### Objectives

1. **Enable new context engine for beta users**
2. **Monitor cache hit rates and assembly times**
3. **Tune budget algorithm elasticity parameters**
4. **Configure real embedding service**

### Pre-Beta Checklist

- [ ] All beta users selected (can use user attribute or feature flag)
- [ ] OpenAI API key configured for embeddings
- [ ] Profile rollup executed for beta users
- [ ] Monitoring dashboard configured
- [ ] Alert thresholds set (cache miss rate, dirty queue length)
- [ ] Runbook updated with beta-specific procedures

### Beta User Selection Strategy

```typescript
// Select beta users by criteria
async function getBetaUsers(criteria: {
  userIds?: string[];  // Explicit list
  percentage?: number;    // Percentage of active users (e.g., 5%)
  criteria?: (user: any) => boolean;  // Custom selection function
}): Promise<string[]> {

  const where: any = { isActive: true };

  if (criteria.userIds) {
    where.id = { in: criteria.userIds };
  } else if (criteria.percentage) {
    const totalUsers = await prisma.user.count({ where });
    const limit = Math.ceil(totalUsers * (criteria.percentage / 100));
    const users = await prisma.user.findMany({
      where,
      take: limit,
      orderBy: { createdAt: 'asc' }  // Early adopters first
    });
    where.id = { in: users.map(u => u.id) };
  } else if (criteria.criteria) {
    // Custom function determines eligibility
    const allUsers = await prisma.user.findMany({ where });
    const eligible = allUsers.filter(criteria.criteria);
    where.id = { in: eligible.map(u => u.id) };
  }

  const users = await prisma.user.findMany({ where });
  return users.map(u => u.id);
}
```

### Environment Configuration for Beta

```bash
# .env.beta

# Enable new context engine for beta group
USE_DYNAMIC_CONTEXT=true

# Beta user selection method
BETA_USER_SELECTION=userIds  # Options: userIds, percentage, criteria

# Beta user IDs (if using userIds method)
BETA_USER_IDS=user-123,user-456,user-789

# Or beta percentage
BETA_USER_PERCENTAGE=5

# Enable detailed logging for beta
DYNAMIC_CONTEXT_LOG_LEVEL=debug

# Embedding service configuration
OPENAI_API_KEY=sk-proj-beta-key
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
EMBEDDING_FALLBACK_TO_MOCK=false

# Monitoring alerts
ALERT_CACHE_MISS_RATE_THRESHOLD=0.30  # Alert if cache hit rate < 70%
ALERT_DIRTY_QUEUE_THRESHOLD=20
ALERT_ASSEMBLY_TIME_THRESHOLD_MS=200

# Feature flag header name
FEATURE_FLAG_HEADER=x-use-dynamic-context-beta
```

### Beta Monitoring Dashboard Configuration

```javascript
// server/src/monitoring/beta-dashboard.js

class BetaMonitoring {
  // Key metrics to track
  metrics = {
    contextAssemblyTime: [],
    cacheHitRate: [],
    newEngineRequests: 0,
    legacyFallbacks: 0,
    errors: []
  };

  // Alert thresholds
  thresholds = {
    cacheMissRate: parseFloat(process.env.ALERT_CACHE_MISS_RATE_THRESHOLD) || 0.30,
    assemblyTime: parseInt(process.env.ALERT_ASSEMBLY_TIME_THRESHOLD_MS) || 200,
    dirtyQueue: parseInt(process.env.ALERT_DIRTY_QUEUE_THRESHOLD) || 20
  };

  async recordMetric(metricName: string, value: number) {
    this.metrics[metricName].push({
      value,
      timestamp: Date.now()
    });

    // Keep only last 1000 data points
    if (this.metrics[metricName].length > 1000) {
      this.metrics[metricName] = this.metrics[metricName].slice(-1000);
    }
  }

  async checkAlerts(): Promise<void> {
    // Check cache hit rate
    if (this.metrics.cacheHitRate.length > 10) {
      const recentHits = this.metrics.cacheHitRate.slice(-100);
      const avgHitRate = recentHits.reduce((sum, m) => sum + m.value, 0) / recentHits.length;

      if (avgHitRate < (1 - this.thresholds.cacheMissRate)) {
        await this.triggerAlert('LOW_CACHE_HIT_RATE', {
          currentRate: avgHitRate,
          threshold: this.thresholds.cacheMissRate,
          recommendation: 'Investigate topic/entity detection accuracy'
        });
      }
    }

    // Check assembly time
    if (this.metrics.contextAssemblyTime.length > 10) {
      const recentTimes = this.metrics.contextAssemblyTime.slice(-100);
      const avgTime = recentTimes.reduce((sum, m) => sum + m.value, 0) / recentTimes.length;

      if (avgTime > this.thresholds.assemblyTime) {
        await this.triggerAlert('SLOW_CONTEXT_ASSEMBLY', {
          avgTime: avgTime,
          threshold: this.thresholds.assemblyTime,
          recommendation: 'Check database query performance or consider more caching'
        });
      }
    }

    // Check dirty queue
    const health = await ContextEngine.getHealth();
    if (health.stats?.dirtyBundles > this.thresholds.dirtyQueue) {
      await this.triggerAlert('HIGH_DIRTY_QUEUE', {
        count: health.stats.dirtyBundles,
        threshold: this.thresholds.dirtyQueue,
        recommendation: 'Invalidation queue may be processing slowly'
      });
    }
  }

  async triggerAlert(alertType: string, details: any): Promise<void> {
    console.log(`[BETA ALERT] ${alertType}:`, details);

    // Send to monitoring service (e.g., PagerDuty, Slack, email)
    await this.sendToMonitoringService({
      alertType,
      severity: this.getSeverity(alertType),
      details,
      timestamp: new Date()
    });
  }

  getSeverity(alertType: string): 'critical' | 'warning' | 'info' {
    const critical = ['LOW_CACHE_HIT_RATE', 'HIGH_DIRTY_QUEUE'];
    return critical.includes(alertType) ? 'critical' : 'warning';
  }
}
```

### Beta Rollout Procedure

```bash
# 1. Deploy code with beta environment variables
# Update .env to enable dynamic context
git checkout -b beta-deploy
# Copy .env.beta to .env
git add .env
git commit -m "Enable dynamic context for beta rollout"

# Deploy to production
# Using your deployment tool (e.g., kubectl, Docker, PM2)
npm run deploy:production

# 2. Run initial profile rollup for beta users
curl -X POST http://your-api.com/api/v1/context/rollup-all \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"limit": 100}'

# 3. Monitor initial metrics
# Check health endpoint
curl http://your-api.com/api/v1/context/health

# 4. Communicate with beta users
# Send announcement about new features
```

### Beta Success Criteria

| Metric | Target | Definition |
|--------|---------|-------------|
| Cache Hit Rate | > 70% | (freshBundles / totalRequests) * 100 |
| Context Assembly Time | < 150ms | p95 of context assembly calls |
| Error Rate | < 1% | New engine errors / total requests |
| Legacy Fallback Rate | < 10% | Should use new engine 90%+ of time |
| User Satisfaction | N/A | Survey beta users |

---

## Phase 3: Full Migration (Production)

### Objectives

1. **Enable new context engine for all users**
2. **Remove legacy fallback paths**
3. **Implement GLMT-4.7 Librarian Loop**
4. **Add js-tiktoken for accurate token estimation**
5. **Optimize for production scale**

### Pre-Production Checklist

- [ ] Beta metrics analyzed and optimizations applied
- [ ] All beta users report positive experience
- [ ] Cache hit rate target (> 85%) achieved
- [ ] Context assembly time target (< 100ms) achieved
- [ ] Error rate target (< 0.5%) achieved
- [ ] Rollback plan documented and tested
- [ ] Monitoring alerts configured for production
- [ ] On-call team briefed on production deployment
- [ ] Load testing completed for 1000 concurrent users

### Production Environment Configuration

```bash
# .env.production

# Enable new context engine for all users
USE_DYNAMIC_CONTEXT=true

# Embedding service (production)
OPENAI_API_KEY=sk-prod-key
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
EMBEDDING_FALLBACK_TO_MOCK=false

# Budget configuration
MAX_CONTEXT_TOKENS=12000
MIN_TOKENS_L0_IDENTITY=150
MIN_TOKENS_L1_PREFS=100
DEFAULT_ELASTICITY_MULTIPLIER=1.0

# Warmup configuration
WARMUP_ENABLED=true
WARMUP_PREDICTION_THRESHOLD=0.15
WARMUP_MAX_PREDICTIONS=8
WARMUP_BATCH_SIZE=5

# Rollup configuration
ROLLUP_ENABLED=true
ROLLUP_BATCH_SIZE=100
ROLLUP_MIN_ACUS_FOR_TOPIC=10
ROLLUP_MIN_ACUS_FOR_ENTITY=5
ROLLUP_INTERVAL_HOURS=6

# Invalidation configuration
INVALIDATION_QUEUE_SIZE=100
INVALIDATION_BATCH_SIZE=20
INVALIDATION_PROCESS_INTERVAL_MS=5000

# Monitoring
CONTEXT_HEALTH_CHECK_ENABLED=true
CONTEXT_METRICS_EXPORT=prometheus
CONTEXT_ALERT_DIRTY_QUEUE_THRESHOLD=50
CONTEXT_ALERT_CACHE_MISS_RATE_THRESHOLD=0.20
```

### Production Deployment Procedure

```bash
# 1. Deploy production configuration
# Update .env with production values
git checkout -b production-deploy
cp .env.production .env
git add .env
git commit -m "Enable full production context engine"
git tag -a v1.0.0-context-engine

# 2. Blue-green deployment (recommended)
# Deploy to new environment (green)
git push origin production

# Wait for health checks
# Monitor metrics for 15 minutes
curl http://green-api.com/api/v1/context/health

# If healthy, switch traffic
# Update load balancer to route to green environment

# 3. Database optimization for production scale

# Run ANALYZE on all context-related tables
psql $DATABASE_URL -c "ANALYZE topic_profiles;"
psql $DATABASE_URL -c "ANALYZE entity_profiles;"
psql $DATABASE_URL -c "ANALYZE context_bundles;"
psql $DATABASE_URL -c "ANALYZE client_presence;"
psql $DATABASE_URL -c "ANALYZE atomic_chat_units;"
psql $DATABASE_URL -c "VACUUM ANALYZE;"

# 4. Configure connection pooling

# Using pgBouncer (recommended)
# Update DATABASE_URL to point to pgBouncer
DATABASE_URL=postgres://user:pass@pgbouncer:6432/host/dbname

# 5. Configure horizontal scaling

# Kubernetes deployment example
kubectl apply -f k8s/deployment.yaml

# 6. Configure monitoring

# Prometheus metrics endpoint
# Expose metrics at /metrics endpoint
# Set up Grafana dashboards
# Configure alerting
```

### Production Load Testing

```bash
# Using Artillery or k6
# Test context assembly endpoint
artillery run tests/context-assembly.yml

# Test AI chat endpoint
artillery run tests/ai-chat.yml

# Verify target metrics
# 1000 concurrent users
# < 100ms p95 latency
# < 1% error rate
```

---

## Environment Configuration

### Required Variables

| Variable | Default | Description |
|-----------|----------|-------------|
| `DATABASE_URL` | Required | PostgreSQL connection string |
| `USE_DYNAMIC_CONTEXT` | `false` | Enable new context engine |
| `OPENAI_API_KEY` | Required (if not using mock embeddings) | API key for embeddings and GPT-4o-mini |
| `EMBEDDING_MODEL` | `text-embedding-3-small` | Embedding model to use |
| `EMBEDDING_DIMENSIONS` | `1536` | Embedding vector dimensions |
| `EMBEDDING_FALLBACK_TO_MOCK` | `true` | Use mock vectors if no API key |
| `MAX_CONTEXT_TOKENS` | `12000` | Default user context window size |
| `NODE_ENV` | `production` | Environment (development/staging/production) |

### Optional Variables (with defaults)

| Variable | Default | Description |
|-----------|----------|-------------|
| `WARMUP_ENABLED` | `true` | Enable proactive warmup |
| `WARMUP_MAX_PREDICTIONS` | `8` | Max predictions to process |
| `ROLLUP_ENABLED` | `true` | Enable profile rollup |
| `ROLLUP_INTERVAL_HOURS` | `6` | Hours between rollup jobs |
| `INVALIDATION_PROCESS_INTERVAL_MS` | `5000` | ms between queue processing |
| `DYNAMIC_CONTEXT_LOG_LEVEL` | `info` | Logging verbosity (debug, info, warn, error) |

---

## Rollback Strategies

### Rollback Triggers

| Condition | Action |
|-----------|--------|
| Cache hit rate < 60% for > 1 hour | Roll back to legacy context |
| Error rate > 5% for > 15 minutes | Roll back to legacy context |
| Assembly time p95 > 300ms | Investigate and consider rollback |
| Database errors > 1% of queries | Roll back and investigate |

### Rollback Procedure

```bash
# 1. Disable new context engine immediately
# Update environment variable
kubectl set env deployment/DYNAMIC_CONTEXT_ENV=false

# 2. Force pod restart to apply changes
kubectl rollout restart deployment/context-engine

# 3. Verify rollback
# Check health endpoint
curl http://your-api.com/api/v1/context/health

# Should show:
# {
#   "status": "healthy",
#   "newEngineAvailable": true,  // But disabled by env var
#   "oldEngineAvailable": true,
#   ...
# }

# 4. Investigate logs
kubectl logs deployment/context-engine --tail=1000

# 5. Analyze incident
# Create incident report
# Document root cause
# Plan fix

# 6. Communicate with stakeholders
# Send incident notification
# Provide ETA for resolution
```

### Feature Flag Rollback

```bash
# Instant rollback via environment variable
curl -X PATCH https://your-api.com/admin/config \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "feature": "DYNAMIC_CONTEXT_ENGINE",
    "enabled": false,
    "reason": "Performance degradation"
  }'
```

---

## Deployment Procedures

### Development Environment

```bash
# 1. Start with default configuration
npm run dev

# Server will start with:
# - USE_DYNAMIC_CONTEXT=false (legacy engine only)
# - Mock embeddings (for development)

# 2. Test new engine locally
# Set environment variable
export USE_DYNAMIC_CONTEXT=true

# Start dev server
npm run dev

# 3. Verify implementation
# Check health endpoint
curl http://localhost:3000/api/v1/context/health

# Test presence update
curl -X POST http://localhost:3000/api/v1/context/presence/user-123 \
  -H "Content-Type: application/json" \
  -d '{
    "deviceId": "dev-device-1",
    "localTime": "2026-02-11T12:30:00Z",
    "activeConversationId": "conv-456"
  }'

# Test AI chat with new engine
curl -X POST http://localhost:3000/api/v1/ai/chat \
  -H "Content-Type: application/json" \
  -H "x-use-dynamic-context: true" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "conversationId": "conv-456",
    "userId": "user-123"
  }'
```

### Production Deployment (Docker)

```dockerfile
# server/Dockerfile

FROM node:18-alpine

WORKDIR /app

# Copy package files
COPY package*.json ./
RUN npm ci --only=production

# Copy source code
COPY . .

# Set environment variables
ENV NODE_ENV=production
ENV USE_DYNAMIC_CONTEXT=true

# Health check
HEALTHCHECK --interval=30s --timeout=3s \
  CMD node -e "require('http').get('http://localhost:3000/api/v1/context/health')" || process.exit(1)"

# Start application
CMD ["node", "server/dist/index.js"]

EXPOSE 3000
```

```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: context-engine
spec:
  replicas: 3
  selector:
    matchLabels:
      app: context-engine
  template:
    metadata:
      labels:
        app: context-engine
    spec:
      containers:
      - name: context-engine
        image: your-registry/context-engine:v1.0.0
        ports:
          - containerPort: 3000
        env:
          - name: DATABASE_URL
            valueFrom:
              secretKeyRef:
                name: database-url
          - name: USE_DYNAMIC_CONTEXT
            value: "true"
          - name: OPENAI_API_KEY
            valueFrom:
              secretKeyRef:
                name: openai-api-key
          - name: MAX_CONTEXT_TOKENS
            value: "12000"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /api/v1/context/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/v1/context/health
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: context-engine-service
spec:
  selector:
    app: context-engine
  ports:
    - protocol: TCP
      port: 3000
      targetPort: 3000
  type: LoadBalancer
```

---

**Document End**

Refer to `IMPLEMENTATION_GUIDE_MASTER.md` for overview and other implementation documents.
